{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOI/uykngoiqWuW6ZkZ9Q02",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ubvw/text-to-sql/blob/main/text_to_sql_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4BtaJfj2f4r"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    %pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    %pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    %pip install --no-deps unsloth\n",
        "%pip install transformers==4.55.4\n",
        "%pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import unsloth\n",
        "    import xformers\n",
        "    import trl\n",
        "    import peft\n",
        "    import accelerate\n",
        "    import bitsandbytes\n",
        "    import triton\n",
        "\n",
        "    print(\"All packages imported successfully.\")\n",
        "\n",
        "    print(f\"Unsloth version: {unsloth.__version__}\")\n",
        "    print(f\"Xformers version: {xformers.__version__}\")\n",
        "    # Add checks for other package versions if needed\n",
        "    print(f\"trl version: {trl.__version__}\")\n",
        "    print(f\"peft version: {peft.__version__}\")\n",
        "    print(f\"accelerate version: {accelerate.__version__}\")\n",
        "    print(f\"bitsandbytes version: {bitsandbytes.__version__}\")\n",
        "    print(f\"triton version: {triton.__version__}\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing a package: {e}\")\n",
        "    print(\"Some packages might not have been installed correctly.\")"
      ],
      "metadata": {
        "id": "WjrItJ062kIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ],
      "metadata": {
        "id": "h1lH0d2o4bp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "ix9GHUou5BB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Company database: {}\n",
        "\n",
        "### Input:\n",
        "SQL Prompt: {}\n",
        "\n",
        "### Response:\n",
        "SQL: {}\n",
        "\n",
        "Explanation: {}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "  company_databases = examples[\"sql_context\"]\n",
        "  prompts = examples[\"sql_prompt\"]\n",
        "  sqls = examples[\"sql\"]\n",
        "  explanations = examples[\"sql_explanation\"]\n",
        "  texts = []\n",
        "  for company_database, prompt, sql, explanation in zip(company_databases, prompts, sqls, explanations):\n",
        "    text = alpaca_prompt.format(company_database, prompt, sql, explanation) + EOS_TOKEN\n",
        "    texts.append(text)\n",
        "  return {\"text\" : texts}\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n"
      ],
      "metadata": {
        "id": "b-mrZFOkCcz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['texts']"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ao7iKiB6CjS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "haSp4eZKCtSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")"
      ],
      "metadata": {
        "id": "bsxkTvDCIsKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
        "if gguf_files:\n",
        "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
        "    print(f\"Downloading: {gguf_file}\")\n",
        "    files.download(gguf_file)"
      ],
      "metadata": {
        "id": "tTx4WfJjNmIw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}